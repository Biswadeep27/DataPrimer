{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\tthere.\n",
      "Hey\\tthere.\n"
     ]
    }
   ],
   "source": [
    "#raw string - r\n",
    "print('Hey\\tthere.')\n",
    "print(r'Hey\\tthere.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_search = '''\n",
    "abcdefghijklmnopqurtuvwxyzaabc\n",
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "1234567890\n",
    "\n",
    "Ha HaHa\n",
    "\n",
    "MetaCharacters (Need to be escaped):\n",
    ". ^ $ * + ? { } [ ] \\ | ( )\n",
    "\n",
    "coreyms.com\n",
    "\n",
    "321-555-4321\n",
    "123.555.1234\n",
    "123*555*1234\n",
    "800-555-1234\n",
    "900-555-1234\n",
    "\n",
    "Mr. Schafer\n",
    "Mr Smith\n",
    "Ms Davis\n",
    "Mrs. Robinson\n",
    "Mr. T\n",
    "Mr. Raj_\n",
    "Mr AryanAryan Aryan\n",
    "Mrs Jane\n",
    "Ms. Izz\n",
    "Ms.. TGND\n",
    "Mr\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 4), match='abc'>\n",
      "<re.Match object; span=(28, 31), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'abc')\n",
    "\n",
    "matches = pattern.finditer(text_to_search)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_search[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(143, 154), match='coreyms.com'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'coreyms\\.com')\n",
    "\n",
    "matches = pattern.finditer(text_to_search)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='With'>\n",
      "<re.Match object; span=(29, 45), match='responsibilities'>\n"
     ]
    }
   ],
   "source": [
    "senetence = 'With great power comes great responsibilities'\n",
    "\n",
    "st_pattern, end_pattern = re.compile(r'^With'), re.compile(r'responsibilities$')\n",
    "\n",
    "st_matches, en_matches = st_pattern.finditer(senetence), end_pattern.finditer(senetence)\n",
    "\n",
    "for match in st_matches:\n",
    "    print(match)\n",
    "\n",
    "for match in en_matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(288, 293), match='Aryan'>\n",
      "<re.Match object; span=(294, 299), match='Aryan'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'Aryan\\b')\n",
    "\n",
    "matches = pattern.finditer(text_to_search)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='\\n'>\n",
      "<re.Match object; span=(31, 32), match='\\n'>\n",
      "<re.Match object; span=(58, 59), match='\\n'>\n",
      "<re.Match object; span=(69, 70), match='\\n'>\n",
      "<re.Match object; span=(70, 71), match='\\n'>\n",
      "<re.Match object; span=(73, 74), match=' '>\n",
      "<re.Match object; span=(78, 79), match='\\n'>\n",
      "<re.Match object; span=(79, 80), match='\\n'>\n",
      "<re.Match object; span=(94, 95), match=' '>\n",
      "<re.Match object; span=(95, 96), match='('>\n",
      "<re.Match object; span=(100, 101), match=' '>\n",
      "<re.Match object; span=(103, 104), match=' '>\n",
      "<re.Match object; span=(106, 107), match=' '>\n",
      "<re.Match object; span=(114, 115), match=')'>\n",
      "<re.Match object; span=(115, 116), match=':'>\n",
      "<re.Match object; span=(116, 117), match='\\n'>\n",
      "<re.Match object; span=(117, 118), match='.'>\n",
      "<re.Match object; span=(118, 119), match=' '>\n",
      "<re.Match object; span=(119, 120), match='^'>\n",
      "<re.Match object; span=(120, 121), match=' '>\n",
      "<re.Match object; span=(121, 122), match='$'>\n",
      "<re.Match object; span=(122, 123), match=' '>\n",
      "<re.Match object; span=(123, 124), match='*'>\n",
      "<re.Match object; span=(124, 125), match=' '>\n",
      "<re.Match object; span=(125, 126), match='+'>\n",
      "<re.Match object; span=(126, 127), match=' '>\n",
      "<re.Match object; span=(127, 128), match='?'>\n",
      "<re.Match object; span=(128, 129), match=' '>\n",
      "<re.Match object; span=(129, 130), match='{'>\n",
      "<re.Match object; span=(130, 131), match=' '>\n",
      "<re.Match object; span=(131, 132), match='}'>\n",
      "<re.Match object; span=(132, 133), match=' '>\n",
      "<re.Match object; span=(133, 134), match='['>\n",
      "<re.Match object; span=(134, 135), match=' '>\n",
      "<re.Match object; span=(135, 136), match=']'>\n",
      "<re.Match object; span=(136, 137), match=' '>\n",
      "<re.Match object; span=(137, 138), match='\\\\'>\n",
      "<re.Match object; span=(138, 139), match=' '>\n",
      "<re.Match object; span=(139, 140), match='|'>\n",
      "<re.Match object; span=(140, 141), match=' '>\n",
      "<re.Match object; span=(141, 142), match='('>\n",
      "<re.Match object; span=(142, 143), match=' '>\n",
      "<re.Match object; span=(143, 144), match=')'>\n",
      "<re.Match object; span=(144, 145), match='\\n'>\n",
      "<re.Match object; span=(145, 146), match='\\n'>\n",
      "<re.Match object; span=(153, 154), match='.'>\n",
      "<re.Match object; span=(157, 158), match='\\n'>\n",
      "<re.Match object; span=(158, 159), match='\\n'>\n",
      "<re.Match object; span=(162, 163), match='-'>\n",
      "<re.Match object; span=(166, 167), match='-'>\n",
      "<re.Match object; span=(171, 172), match='\\n'>\n",
      "<re.Match object; span=(175, 176), match='.'>\n",
      "<re.Match object; span=(179, 180), match='.'>\n",
      "<re.Match object; span=(184, 185), match='\\n'>\n",
      "<re.Match object; span=(188, 189), match='*'>\n",
      "<re.Match object; span=(192, 193), match='*'>\n",
      "<re.Match object; span=(197, 198), match='\\n'>\n",
      "<re.Match object; span=(201, 202), match='-'>\n",
      "<re.Match object; span=(205, 206), match='-'>\n",
      "<re.Match object; span=(210, 211), match='\\n'>\n",
      "<re.Match object; span=(214, 215), match='-'>\n",
      "<re.Match object; span=(218, 219), match='-'>\n",
      "<re.Match object; span=(223, 224), match='\\n'>\n",
      "<re.Match object; span=(224, 225), match='\\n'>\n",
      "<re.Match object; span=(227, 228), match='.'>\n",
      "<re.Match object; span=(228, 229), match=' '>\n",
      "<re.Match object; span=(236, 237), match='\\n'>\n",
      "<re.Match object; span=(239, 240), match=' '>\n",
      "<re.Match object; span=(245, 246), match='\\n'>\n",
      "<re.Match object; span=(248, 249), match=' '>\n",
      "<re.Match object; span=(254, 255), match='\\n'>\n",
      "<re.Match object; span=(258, 259), match='.'>\n",
      "<re.Match object; span=(259, 260), match=' '>\n",
      "<re.Match object; span=(268, 269), match='\\n'>\n",
      "<re.Match object; span=(271, 272), match='.'>\n",
      "<re.Match object; span=(272, 273), match=' '>\n",
      "<re.Match object; span=(274, 275), match='\\n'>\n",
      "<re.Match object; span=(277, 278), match='.'>\n",
      "<re.Match object; span=(278, 279), match=' '>\n",
      "<re.Match object; span=(282, 283), match='_'>\n",
      "<re.Match object; span=(283, 284), match='\\n'>\n",
      "<re.Match object; span=(286, 287), match='.'>\n",
      "<re.Match object; span=(287, 288), match=' '>\n",
      "<re.Match object; span=(298, 299), match=' '>\n",
      "<re.Match object; span=(304, 305), match='\\n'>\n"
     ]
    }
   ],
   "source": [
    "#inside the character set ^ negates the match\n",
    "pattern = re.compile(r'[^a-zA-Z0-9]')\n",
    "\n",
    "matches = pattern.finditer(text_to_search)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(225, 236), match='Mr. Schafer'>\n",
      "<re.Match object; span=(237, 245), match='Mr Smith'>\n",
      "<re.Match object; span=(246, 254), match='Ms Davis'>\n",
      "<re.Match object; span=(255, 268), match='Mrs. Robinson'>\n",
      "<re.Match object; span=(269, 274), match='Mr. T'>\n",
      "<re.Match object; span=(275, 282), match='Mr. Raj'>\n",
      "<re.Match object; span=(284, 297), match='Mr AryanAryan'>\n",
      "<re.Match object; span=(304, 312), match='Mrs Jane'>\n",
      "<re.Match object; span=(313, 320), match='Ms. Izz'>\n",
      "<re.Match object; span=(321, 330), match='Ms.. TGND'>\n"
     ]
    }
   ],
   "source": [
    "# inside the [] - placed at the beginning or end will match the exact literal - but if put between values then it can match a range of values\n",
    "#r'M(r|s|rs)\\.?\\s\\w*[a-zA-Z]'\n",
    "pattern = re.compile(r'M(r|s|rs)\\.{0,2}\\s\\w*[a-zA-Z]')\n",
    "\n",
    "matches = pattern.finditer(text_to_search)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'800-555-5669': 1, '900-555-9340': 1, '800-555-6771': 1, '900-555-3205': 1, '800-555-6089': 1, '880-555-8319': 1, '998-555-7385': 1, '800-555-7100': 1, '903-555-8277': 1, '900-555-5118': 1, '905-555-1630': 1, '884-555-8444': 1, '904-555-8559': 1, '889-555-7393': 1, '900-555-5428': 1, '932-555-7724': 1, '800-555-8810': 1, '903-555-9878': 1, '900-555-9598': 1, '866-555-9844': 1, '893-555-9832': 1, '926-555-8735': 1, '895-555-3539': 1, '874-555-3949': 1, '800-555-2420': 1, '936-555-6340': 1, '890-555-5618': 1, '900-555-3567': 1, '974-555-8301': 1, '800-555-3216': 1, '922-555-1773': 1, '852-555-6521': 1, '900-555-7755': 1, '911-555-7535': 1, '800-555-1372': 1, '988-555-6112': 1, '952-555-3089': 1, '900-555-6426': 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "hash_map = {}\n",
    "with open('re_data.txt','r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "    pattern = re.compile(r'[89]\\d\\d[-_*.]\\d\\d\\d[-_*.]\\d{4}')\n",
    "\n",
    "    matches = pattern.finditer(contents)\n",
    "\n",
    "    for match in matches:\n",
    "        s = match.start()\n",
    "        e = match.end()\n",
    "        #print(contents[s:e])\n",
    "        ph_no = contents[s:e]\n",
    "\n",
    "        hash_map[ph_no] = hash_map.get(ph_no,0) + 1\n",
    "\n",
    "\n",
    "print(hash_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 24), match='CoreyMSchafer@gmail.com'>\n",
      "<re.Match object; span=(25, 53), match='corey.schafer@university.edu'>\n",
      "<re.Match object; span=(54, 83), match='corey-321-schafer@my-work.net'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "emails = '''\n",
    "CoreyMSchafer@gmail.com\n",
    "corey.schafer@university.edu\n",
    "corey-321-schafer@my-work.net\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "\n",
    "matches = pattern.finditer(emails)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 2, 'B': 1, 'C': 1, ' ': 1, 'D': 1, 'F': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_map = {}\n",
    "r = \"ABC ADF\"\n",
    "for _c in r:\n",
    "    hash_map[_c] = hash_map.get(_c,0) + 1\n",
    "\n",
    "print(hash_map)\n",
    "\n",
    "d = {'B': 1, 'D': 1, 'A': 2, ' ': 1, 'C': 1, 'F': 1}\n",
    "\n",
    "hash_map == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "google.com\n",
      "coreyms.com\n",
      "youtube.com\n",
      "nasa.gov\n",
      "\n",
      ".com\n",
      ".com\n",
      ".com\n",
      ".gov\n",
      "[('www.', 'google', '.com'), ('', 'coreyms', '.com'), ('', 'youtube', '.com'), ('www.', 'nasa', '.gov')]\n",
      "['https://www.google.com', 'http://coreyms.com', 'https://youtube.com', 'https://www.nasa.gov']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "urls = '''\n",
    "https://www.google.com\n",
    "http://coreyms.com\n",
    "https://youtube.com\n",
    "https://www.nasa.gov\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')\n",
    "\n",
    "subbed_urls = pattern.sub(r'\\2\\3', urls)\n",
    "\n",
    "print(subbed_urls)\n",
    "\n",
    "matches = pattern.finditer(urls)\n",
    "\n",
    "for match in matches:\n",
    "    print(match.group(3))\n",
    "\n",
    "\n",
    "\n",
    "#findall finda all the available groups , if there are no groups then it returns the entire matched string\n",
    "matches = pattern.findall(urls)\n",
    "print(matches)\n",
    "\n",
    "pattern1 = re.compile(r'https?://\\w+\\.\\w+\\.?\\w*')\n",
    "matches1 = pattern1.findall(urls)\n",
    "print(matches1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(21, 25), match='then'>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Start a sentence and then bring it to an end'\n",
    "\n",
    "pattern = re.compile(r'then', re.I)\n",
    "\n",
    "matches = pattern.search(sentence)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\t spark-submit --deploy-mode cluster --name \"k8s_kafka_extract_sparkway\" --conf spark.kubernetes.namespace=daciteamspark --conf spark.kubernetes.authenticate.driver.serviceAccountName=daciteamspark-sa --conf spark.kubernetes.container.image=e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing --conf spark.kubernetes.container.image.pullPolicy=Always --conf spark.hadoop.fs.s3a.path.style.access=true --jars /Dacikubdata/externalJars/kafka/commons-pool2-2.6.2.jar,/Dacikubdata/externalJars/kafka/kafka-clients-2.6.0.jar,/Dacikubdata/externalJars/kafka/lz4-java-1.7.1.jar,/Dacikubdata/externalJars/kafka/scala-library-2.12.10.jar,/Dacikubdata/externalJars/kafka/slf4j-api-1.7.30.jar,/Dacikubdata/externalJars/kafka/snappy-java-1.1.7.3.jar,/Dacikubdata/externalJars/kafka/spark-sql-kafka-0-10_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/spark-tags_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/spark-token-provider-kafka-0-10_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/unused-1.0.0.jar,/Dacikubdata/externalJars/kafka/zstd-jni-1.4.4-7.jar --conf spark.kubernetes.driver.podTemplateFile=/Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/yaml/podtemplate-dev-testing.yaml --conf spark.kubernetes.executor.podTemplateFile=/Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/yaml/podtemplate-dev-testing.yaml --conf \"spark.pyspark.python=/usr/bin/python3\" --conf \"spark.pyspark.driver=/usr/bin/python3\"  --files local:///apps/application/properties/Item_Services_Data_Extract.json,file:///Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/properties/truststore.jks --py-files local:///apps/application/python/item_modernization_utilities.py --executor-memory 3G --driver-memory 2G --num-executors 3 --executor-cores 5 /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/python/Item_Services_KafkaExtract.py\n",
    "\n",
    "22/09/20 13:32:14 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
    "22/09/20 13:32:15 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.\n",
    "22/09/20 13:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "22/09/20 13:32:16 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/commons-pool2-2.6.2.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-dd06458f-a6c6-4b31-88a5-5c70403daa33/commons-pool2-2.6.2.jar...\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/kafka-clients-2.6.0.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-3d0a1c9a-a75f-4614-982a-cdd33b106a59/kafka-clients-2.6.0.jar...\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/lz4-java-1.7.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-32254310-815b-4c50-be90-804a2aeff28b/lz4-java-1.7.1.jar...\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/scala-library-2.12.10.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-1692e6c2-bae6-4aa9-a26e-ce0723c5187e/scala-library-2.12.10.jar...\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/slf4j-api-1.7.30.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-725e03fd-0c65-464d-824b-e5a759679eee/slf4j-api-1.7.30.jar...\n",
    "22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/snappy-java-1.1.7.3.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-ba49b316-a479-4e9f-a5cb-d00d742819ab/snappy-java-1.1.7.3.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-sql-kafka-0-10_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-314fc8d7-56ce-4fe3-b9f0-027f9d647494/spark-sql-kafka-0-10_2.12-3.1.1.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-tags_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-2b0faefd-4359-4516-99e7-731232f3d2f6/spark-tags_2.12-3.1.1.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-token-provider-kafka-0-10_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-1c8b998b-f1f9-408b-867b-b7a01d52bf3d/spark-token-provider-kafka-0-10_2.12-3.1.1.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/unused-1.0.0.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-eb725f15-26fa-4530-98bf-c7d0ee9713c4/unused-1.0.0.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/zstd-jni-1.4.4-7.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-4faa1dad-b42a-4b38-9b7a-690ef4cf3c52/zstd-jni-1.4.4-7.jar...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/properties/truststore.jks to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-d2c234be-8d00-44ad-8fc3-e0f5c5bf05fb/truststore.jks...\n",
    "22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/python/Item_Services_KafkaExtract.py to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-0bacb20e-c2f7-4d8f-894d-4bae3de1623e/Item_Services_KafkaExtract.py...\n",
    "22/09/20 13:32:17 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/apps/current/spark/conf) : log4j.properties,dummy.yaml,spark-defaults.conf_bkp_07072022,spark-env.sh,spark-defaults.conf.0405.tempalte,spark-defaults.conf_bkp_05042022,metrics.properties\n",
    "22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n",
    "\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\n",
    "\t namespace: daciteamspark\n",
    "\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\n",
    "\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\n",
    "\t creation time: 2022-09-20T17:32:17Z\n",
    "\t service account name: daciteamspark-sa\n",
    "\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\n",
    "\t node name: lxdcpwrkrprds19\n",
    "\t start time: 2022-09-20T17:32:17Z\n",
    "\t phase: Pending\n",
    "\t container status:\n",
    "\t\t container name: spark-configmap-secret-dev\n",
    "\t\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t\t container state: waiting\n",
    "\t\t pending reason: ContainerCreating\n",
    "22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n",
    "\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\n",
    "\t namespace: daciteamspark\n",
    "\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\n",
    "\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\n",
    "\t creation time: 2022-09-20T17:32:17Z\n",
    "\t service account name: daciteamspark-sa\n",
    "\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\n",
    "\t node name: lxdcpwrkrprds19\n",
    "\t start time: 2022-09-20T17:32:17Z\n",
    "\t phase: Pending\n",
    "\t container status:\n",
    "\t\t container name: spark-configmap-secret-dev\n",
    "\t\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t\t container state: waiting\n",
    "\t\t pending reason: ContainerCreating\n",
    "22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: Waiting for application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-4175c6835bf51ebb-driver to finish...\n",
    "22/09/20 13:32:19 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n",
    "\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\n",
    "\t namespace: daciteamspark\n",
    "\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\n",
    "\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\n",
    "\t creation time: 2022-09-20T17:32:17Z\n",
    "\t service account name: daciteamspark-sa\n",
    "\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\n",
    "\t node name: lxdcpwrkrprds19\n",
    "\t start time: 2022-09-20T17:32:17Z\n",
    "\t phase: Pending\n",
    "\t container status:\n",
    "\t\t container name: spark-configmap-secret-dev\n",
    "\t\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t\t container state: waiting\n",
    "\t\t pending reason: ContainerCreating\n",
    "22/09/20 13:32:19 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Pending)\n",
    "22/09/20 13:32:20 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Pending)\n",
    "22/09/20 13:32:21 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n",
    "\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\n",
    "\t namespace: daciteamspark\n",
    "\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\n",
    "\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\n",
    "\t creation time: 2022-09-20T17:32:17Z\n",
    "\t service account name: daciteamspark-sa\n",
    "\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\n",
    "\t node name: lxdcpwrkrprds19\n",
    "\t start time: 2022-09-20T17:32:17Z\n",
    "\t phase: Running\n",
    "\t container status:\n",
    "\t\t container name: spark-configmap-secret-dev\n",
    "\t\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t\t container state: running\n",
    "\t\t container started at: 2022-09-20T17:32:20Z\n",
    "22/09/20 13:32:21 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Running)\n",
    "22/09/20 13:32:22 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Running)\n",
    "22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n",
    "\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\n",
    "\t namespace: daciteamspark\n",
    "\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\n",
    "\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\n",
    "\t creation time: 2022-09-20T17:32:17Z\n",
    "\t service account name: daciteamspark-sa\n",
    "\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\n",
    "\t node name: lxdcpwrkrprds19\n",
    "\t start time: 2022-09-20T17:32:17Z\n",
    "\t phase: Failed\n",
    "\t container status:\n",
    "\t\t container name: spark-configmap-secret-dev\n",
    "\t\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t\t container state: terminated\n",
    "\t\t container started at: 2022-09-20T17:32:20Z\n",
    "\t\t container finished at: 2022-09-20T17:32:23Z\n",
    "\t\t exit code: 117\n",
    "\t\t termination reason: Error\n",
    "22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Failed)\n",
    "22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Container final statuses:\n",
    "\n",
    "\n",
    "\t container name: spark-configmap-secret-dev\n",
    "\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
    "\t container state: terminated\n",
    "\t container started at: 2022-09-20T17:32:20Z\n",
    "\t container finished at: 2022-09-20T17:32:23Z\n",
    "\t exit code: 117\n",
    "\t termination reason: Error\n",
    "22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-4175c6835bf51ebb-driver finished\n",
    "22/09/20 13:32:23 INFO ShutdownHookManager: Shutdown hook called\n",
    "22/09/20 13:32:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-0210bd59-89ce-4e11-b6d7-6e49a8101d52\n",
    " Spark Process : SUCCESS\n",
    "2022-09-20 13:32:23.297 - INFO  - Task(MainProcess) - stderr string : Spark Process : SUCCESS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Application status for spark-4146e0251cd24145bcd7dfe44ffd8d78 (phase: Failed)\n",
      "22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Container final statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-configmap-secret-dev\n",
      "\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-09-20T17:03:31Z\n",
      "\t container finished at: 2022-09-20T17:03:34Z\n",
      "\t exit code: 1\n",
      "\t termination reason: Error\n",
      "22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-b59735835bdabd5a-driver finished\n",
      "22/09/20 13:03:34 INFO ShutdownHookManager: Shutdown hook called\n",
      "22/09/20 13:03:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-2025e5e6-105c-420a-9a02-af118723b944\n",
      " Spark Process : SUCCESS\n",
      "2022-09-20 13:03:34.889 - INFO  - Task(MainProcess) - stderr string : Spark Process : SUCCESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exit code: 1\\n\\t\\t termination reason: Error\\n22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Application status for spark-4146e0251cd24145bcd7dfe44ffd8d78 (phase: Failed)\\n22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Container final statuses:\\n\\n\\n\\t container name: spark-configmap-secret-dev\\n\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t container state: terminated\\n\\t container started at: 2022-09-20T17:03:31Z\\n\\t container finished at: 2022-09-20T17:03:34Z\\n\\t exit code: 1\\n\\t termination reason: Error\\n22/09/20 13:03:34 INFO LoggingPodStatusWatcherImpl: Application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-b59735835bdabd5a-driver finished\\n22/09/20 13:03:34 INFO ShutdownHookManager: Shutdown hook called\\n22/09/20 13:03:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-2025e5e6-105c-420a-9a02-af118723b944\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[807:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "it failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "pattern = re.compile(r'exit code:')\n",
    "\n",
    "indices = []\n",
    "matches = pattern.finditer(text)\n",
    "for match in matches:\n",
    "    indices.append(match.end())\n",
    "\n",
    "\n",
    "#print(indices)\n",
    "\n",
    "if indices:\n",
    "    ec_index = indices[-1]\n",
    "\n",
    "    exit_code = text[ec_index:].split('\\n')[0].strip()\n",
    "\n",
    "    print(exit_code)\n",
    "    if int(exit_code) == 0:\n",
    "        print('spark process SUCCESS.')\n",
    "    else:\n",
    "        print('it failed')\n",
    "\n",
    "else:\n",
    "    print('failed to retrieve')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1319, 1329), match='exit code:'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\t spark-submit --deploy-mode cluster --name \"k8s_kafka_extract_sparkway\" --conf spark.kubernetes.namespace=daciteamspark --conf spark.kubernetes.authenticate.driver.serviceAccountName=daciteamspark-sa --conf spark.kubernetes.container.image=e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing --conf spark.kubernetes.container.image.pullPolicy=Always --conf spark.hadoop.fs.s3a.path.style.access=true --jars /Dacikubdata/externalJars/kafka/commons-pool2-2.6.2.jar,/Dacikubdata/externalJars/kafka/kafka-clients-2.6.0.jar,/Dacikubdata/externalJars/kafka/lz4-java-1.7.1.jar,/Dacikubdata/externalJars/kafka/scala-library-2.12.10.jar,/Dacikubdata/externalJars/kafka/slf4j-api-1.7.30.jar,/Dacikubdata/externalJars/kafka/snappy-java-1.1.7.3.jar,/Dacikubdata/externalJars/kafka/spark-sql-kafka-0-10_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/spark-tags_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/spark-token-provider-kafka-0-10_2.12-3.1.1.jar,/Dacikubdata/externalJars/kafka/unused-1.0.0.jar,/Dacikubdata/externalJars/kafka/zstd-jni-1.4.4-7.jar --conf spark.kubernetes.driver.podTemplateFile=/Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/yaml/podtemplate-dev-testing.yaml --conf spark.kubernetes.executor.podTemplateFile=/Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/yaml/podtemplate-dev-testing.yaml --conf \"spark.pyspark.python=/usr/bin/python3\" --conf \"spark.pyspark.driver=/usr/bin/python3\"  --files local:///apps/application/properties/Item_Services_Data_Extract.json,file:///Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/properties/truststore.jks --py-files local:///apps/application/python/item_modernization_utilities.py --executor-memory 3G --driver-memory 2G --num-executors 3 --executor-cores 5 /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/python/Item_Services_KafkaExtract.py\\n\\n22/09/20 13:32:14 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\\n22/09/20 13:32:15 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.\\n22/09/20 13:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n22/09/20 13:32:16 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/commons-pool2-2.6.2.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-dd06458f-a6c6-4b31-88a5-5c70403daa33/commons-pool2-2.6.2.jar...\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/kafka-clients-2.6.0.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-3d0a1c9a-a75f-4614-982a-cdd33b106a59/kafka-clients-2.6.0.jar...\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/lz4-java-1.7.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-32254310-815b-4c50-be90-804a2aeff28b/lz4-java-1.7.1.jar...\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/scala-library-2.12.10.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-1692e6c2-bae6-4aa9-a26e-ce0723c5187e/scala-library-2.12.10.jar...\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/slf4j-api-1.7.30.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-725e03fd-0c65-464d-824b-e5a759679eee/slf4j-api-1.7.30.jar...\\n22/09/20 13:32:16 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/snappy-java-1.1.7.3.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-ba49b316-a479-4e9f-a5cb-d00d742819ab/snappy-java-1.1.7.3.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-sql-kafka-0-10_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-314fc8d7-56ce-4fe3-b9f0-027f9d647494/spark-sql-kafka-0-10_2.12-3.1.1.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-tags_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-2b0faefd-4359-4516-99e7-731232f3d2f6/spark-tags_2.12-3.1.1.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/spark-token-provider-kafka-0-10_2.12-3.1.1.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-1c8b998b-f1f9-408b-867b-b7a01d52bf3d/spark-token-provider-kafka-0-10_2.12-3.1.1.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/unused-1.0.0.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-eb725f15-26fa-4530-98bf-c7d0ee9713c4/unused-1.0.0.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/externalJars/kafka/zstd-jni-1.4.4-7.jar to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-4faa1dad-b42a-4b38-9b7a-690ef4cf3c52/zstd-jni-1.4.4-7.jar...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/properties/truststore.jks to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-d2c234be-8d00-44ad-8fc3-e0f5c5bf05fb/truststore.jks...\\n22/09/20 13:32:17 INFO KubernetesUtils: Uploading file: /Dacikubdata/sandbox/daci/k8s_poc/code/feature/feature-dev-k8s-testing/python/Item_Services_KafkaExtract.py to dest: hdfs:///hdfsData/daci/kuberentes/stgfiles//spark-upload-0bacb20e-c2f7-4d8f-894d-4bae3de1623e/Item_Services_KafkaExtract.py...\\n22/09/20 13:32:17 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/apps/current/spark/conf) : log4j.properties,dummy.yaml,spark-defaults.conf_bkp_07072022,spark-env.sh,spark-defaults.conf.0405.tempalte,spark-defaults.conf_bkp_05042022,metrics.properties\\n22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: State changed, new state:\\n\\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\\n\\t namespace: daciteamspark\\n\\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\\n\\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\\n\\t creation time: 2022-09-20T17:32:17Z\\n\\t service account name: daciteamspark-sa\\n\\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\\n\\t node name: lxdcpwrkrprds19\\n\\t start time: 2022-09-20T17:32:17Z\\n\\t phase: Pending\\n\\t container status:\\n\\t\\t container name: spark-configmap-secret-dev\\n\\t\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t\\t container state: waiting\\n\\t\\t pending reason: ContainerCreating\\n22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: State changed, new state:\\n\\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\\n\\t namespace: daciteamspark\\n\\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\\n\\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\\n\\t creation time: 2022-09-20T17:32:17Z\\n\\t service account name: daciteamspark-sa\\n\\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\\n\\t node name: lxdcpwrkrprds19\\n\\t start time: 2022-09-20T17:32:17Z\\n\\t phase: Pending\\n\\t container status:\\n\\t\\t container name: spark-configmap-secret-dev\\n\\t\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t\\t container state: waiting\\n\\t\\t pending reason: ContainerCreating\\n22/09/20 13:32:18 INFO LoggingPodStatusWatcherImpl: Waiting for application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-4175c6835bf51ebb-driver to finish...\\n22/09/20 13:32:19 INFO LoggingPodStatusWatcherImpl: State changed, new state:\\n\\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\\n\\t namespace: daciteamspark\\n\\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\\n\\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\\n\\t creation time: 2022-09-20T17:32:17Z\\n\\t service account name: daciteamspark-sa\\n\\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\\n\\t node name: lxdcpwrkrprds19\\n\\t start time: 2022-09-20T17:32:17Z\\n\\t phase: Pending\\n\\t container status:\\n\\t\\t container name: spark-configmap-secret-dev\\n\\t\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t\\t container state: waiting\\n\\t\\t pending reason: ContainerCreating\\n22/09/20 13:32:19 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Pending)\\n22/09/20 13:32:20 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Pending)\\n22/09/20 13:32:21 INFO LoggingPodStatusWatcherImpl: State changed, new state:\\n\\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\\n\\t namespace: daciteamspark\\n\\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\\n\\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\\n\\t creation time: 2022-09-20T17:32:17Z\\n\\t service account name: daciteamspark-sa\\n\\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\\n\\t node name: lxdcpwrkrprds19\\n\\t start time: 2022-09-20T17:32:17Z\\n\\t phase: Running\\n\\t container status:\\n\\t\\t container name: spark-configmap-secret-dev\\n\\t\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t\\t container state: running\\n\\t\\t container started at: 2022-09-20T17:32:20Z\\n22/09/20 13:32:21 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Running)\\n22/09/20 13:32:22 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Running)\\n22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: State changed, new state:\\n\\t pod name: k8skafkaextractsparkway-4175c6835bf51ebb-driver\\n\\t namespace: daciteamspark\\n\\t labels: spark-app-selector -> spark-cf2fa92c83e749be9c7e41287221fc12, spark-role -> driver\\n\\t pod uid: 3e042759-fc2a-455c-b1dd-634e843cc3d7\\n\\t creation time: 2022-09-20T17:32:17Z\\n\\t service account name: daciteamspark-sa\\n\\t volumes: checkpointpvc, hadoop-properties, pod-template-volume, spark-local-dir-1, spark-conf-volume-driver, kube-api-access-2xwfc\\n\\t node name: lxdcpwrkrprds19\\n\\t start time: 2022-09-20T17:32:17Z\\n\\t phase: Failed\\n\\t container status:\\n\\t\\t container name: spark-configmap-secret-dev\\n\\t\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t\\t container state: terminated\\n\\t\\t container started at: 2022-09-20T17:32:20Z\\n\\t\\t container finished at: 2022-09-20T17:32:23Z\\n\\t\\t exit code: 1\\n\\t\\t termination reason: Error\\n22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Application status for spark-cf2fa92c83e749be9c7e41287221fc12 (phase: Failed)\\n22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Container final statuses:\\n\\n\\n\\t container name: spark-configmap-secret-dev\\n\\t container image: e-din-docker-local.docker.lowes.com/daci/k8s_poc:feature-dev-k8s-testing\\n\\t container state: terminated\\n\\t container started at: 2022-09-20T17:32:20Z\\n\\t container finished at: 2022-09-20T17:32:23Z\\n\\t exit code: 1\\n\\t termination reason: Error\\n22/09/20 13:32:23 INFO LoggingPodStatusWatcherImpl: Application k8s_kafka_extract_sparkway with submission ID daciteamspark:k8skafkaextractsparkway-4175c6835bf51ebb-driver finished\\n22/09/20 13:32:23 INFO ShutdownHookManager: Shutdown hook called\\n22/09/20 13:32:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-0210bd59-89ce-4e11-b6d7-6e49a8101d52\\n Spark Process : SUCCESS\\n2022-09-20 13:32:23.297 - INFO  - Task(MainProcess) - stderr string : Spark Process : SUCCESS\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1a85674b7b9c706837e67e471571332ebf1f79fce68aebc7a712558674e3b4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
