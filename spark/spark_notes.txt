spark : a Definitive guide:
===============================

Narrow transformation — In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().

Wide transformation — In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey and reducebyKey.


Actions
Transformations create RDDs from each other, but when we want to work with the actual dataset, at that point action is performed. When the action is triggered after the result, new RDD is not formed like transformation. Thus, actions are RDD operations that give non-RDD values. The values of action are stored to drivers or to the external storage system. It brings laziness of RDD into motion.
Spark drivers and external storage system store the value of action. It brings laziness of RDD into motion.

An action is one of the ways of sending data from Executer to the driver. Executors are agents that are responsible for executing a task. While the driver is a JVM process that coordinates workers and execution of the task.

Spark workflow:
-----------------

STEP 1: The client submits spark user application code. When an application code is submitted, the driver implicitly converts user code that contains transformations and actions into a logically directed acyclic graph called DAG. At this stage, it also performs optimizations such as pipelining transformations.

STEP 2: After that, it converts the logical graph called DAG into physical execution plan with many stages. After converting into a physical execution plan, it creates physical execution units called tasks under each stage. Then the tasks are bundled and sent to the cluster.

STEP 3: Now the driver talks to the cluster manager and negotiates the resources. Cluster manager launches executors in worker nodes on behalf of the driver. At this point, the driver will send the tasks to the executors based on data placement. When executors start, they register themselves with drivers. So, the driver will have a complete view of executors that are executing the task.

STEP 4: During the course of execution of tasks, driver program will monitor the set of executors that runs. Driver node also schedules future tasks based on data placement. 


transformation-> action -> job -> stage -> task

1. Application : It could be single command or combination of commands with complex logic. When code is submitted to spark for execution, the application kicks off.

2. Job : When an application is submitted to spark, driver process converts the code  into job.

3. Stage : Jobs are divided into stages. If the application code demands shuffling data accross nodes, new stage is created. Number of stages are determined by the 
number of shuffling operation.

4. Task : Stages are further divided into multiple tasks, In a stage, all the tasks would execute same logic. Each task would process one partition at a time. So number of partition in the distributed cluster, would determine the number of task at each stage.

1 task = 1 slot = 1 core =  1 partition (Physical execution unit)

Executor Memory :

for a 32 GB RAM(On-Heap Memory) on each executor ->

~300 MB -> Reserved Memory
40 % ~ 13 GB -> User memory -> to store metadata and other objects 
60 % ~ 19 GB -> Unified Memory

Unified Memory = 50% Storage Memory (Partitons/persist) + 50 % Execution/Processing Memory (by default but can be configured.)


Executors  have off-heap memory as well (Disks)



f"{str(((h+(d//60))%24) + ((m + (d%60))//60))}:{str(((m + (d%60))%60))}"


==============================
Spark Joining strategies:
------

1. Hash Join: Hash Join is performed by first creating a Hash Table based on join_key of smaller relation and then looping over larger relation to match the hashed join_key values. Also, this is only supported for ‘=’ join. In spark, Hash Join plays a role at per node level and the strategy is used to join partitions available on the node.

2. Broadcast Hash Join: In broadcast hash join, copy of one of the join relations are being sent to all the worker nodes and it saves shuffling cost. This is useful when you are joining a large relation with a smaller one. It is also known as map-side join(associating worker nodes with mappers).

Spark deploys this join strategy when the size of one of the join relations is less than the threshold values(default 10 M). The spark property which defines this threshold is spark.sql.autoBroadcastJoinThreshold(configurable).

Things to Note:

#The broadcasted relation should fit completely into the memory of each executor as well as the driver. In Driver, because driver will start the data transfer.
#Only supported for ‘=’ join.
#Supported for all join types(inner, left, right) except full outer joins.
#When the broadcast size is small, it is usually faster than other join strategies.
#Copy of relation is broadcasted over the network. Therefore, being a network-intensive operation could cause out of memory errors or performance issues when broadcast size is big(for instance, when explicitly specified to use broadcast join/changes in the default threshold)
#You can’t make changes to the broadcasted relation, after broadcast. Even if you do, they won’t be available to the worker nodes(because the copy is already shipped).


3. Shuffle hash join: Shuffle Hash Join involves moving data with the same value of join key in the same executor node followed by Hash Join(explained above). Using the join condition as output key, data is shuffled amongst executor nodes and in the last step, data is combined using Hash Join, as we know data of the same key will be present in the same executor. (= joins except full outer join)

4. Shuffle sort-merge join: Sort join involves, first sorting the relations based on join keys and then merging both the datasets(think of merge step of merge sort).

Shuffle sort-merge join involves, shuffling of data to get the same join_key with the same worker, and then performing sort-merge join operation at the partition level in the worker nodes. (all = joins)

5. Cartesian Join: In this strategy, the cartesian product(similar to SQL) of the two relations is calculated to evaluate join.

6. Broadcast nested loop join : Think of this as a nested loop comparison of both the relations. As you can see, this can be a very slow strategy. This is generally, a fallback option when no other join type can be applied. Spark handles this using BroadcastNestedLoopJoinExec operator that broadcasts the appropriate side of the query, so you can think that at least some chunk of results will be broadcasted to improve performance. (all joins i.e = > < >= <=)


How spark selects join strategy?
--
If it is an ‘=’ join:
Look at the join hints, in the following order:
1. Broadcast Hint: Pick broadcast hash join if the join type is supported.
2. Sort merge hint: Pick sort-merge join if join keys are sortable.
3. shuffle hash hint: Pick shuffle hash join if the join type is supported.
4. shuffle replicate NL hint: pick cartesian product if join type is inner like.

If there is no hint or the hints are not applicable
1. Pick broadcast hash join if one side is small enough to broadcast, and the join type is supported.
2. Pick shuffle hash join if one side is small enough to build the local hash map, and is much smaller than the other side, and spark.sql.join.preferSortMergeJoin is false.
3. Pick sort-merge join if join keys are sortable.
4. Pick cartesian product if join type is inner .
5. Pick broadcast nested loop join as the final solution. It may OOM but there is no other choice.

If it’s not ‘=’ join:
Look at the join hints, in the following order:
1. broadcast hint: pick broadcast nested loop join.
2. shuffle replicate NL hint: pick cartesian product if join type is inner like.

If there is no hint or the hints are not applicable
1. Pick broadcast nested loop join if one side is small enough to broadcast.
2. Pick cartesian product if join type is inner like.
3. Pick broadcast nested loop join as the final solution. It may OOM but we don’t have any other choice.


Hardware Provisioning:
=========================

A. Storage Systems : Because most Spark jobs will likely have to read input data from an external storage system (e.g. the Hadoop File System, or HBase), it is important to place it as close to this system as possible. We recommend the following:

- If at all possible, run Spark on the same nodes as HDFS. Alternatively, you can run Hadoop and Spark on a common cluster manager like Mesos or Hadoop YARN.
- If this is not possible, run Spark on different nodes in the same local-area network as HDFS.
- For low-latency data stores like HBase, it may be preferable to run computing jobs on different nodes than the storage system to avoid interference.

B. Local Disks: While Spark can perform a lot of its computation in memory, it still uses local disks to store data that doesn’t fit in RAM, as well as to preserve intermediate output between stages. We recommend having 4-8 disks per node, configured without RAID (just as separate mount points).

C. Memory: In general, Spark can run well with anywhere from 8 GiB to hundreds of gigabytes of memory per machine. In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the rest for the operating system and buffer cache. 

Finally, note that the Java VM does not always behave well with more than 200 GiB of RAM. If you purchase machines with more RAM than this, you can launch multiple executors in a single node. In Spark’s standalone mode, a worker is responsible for launching multiple executors according to its available memory and cores, and each executor will be launched in a separate Java VM.

D. Network: In our experience, when the data is in memory, a lot of Spark applications are network-bound. Using a 10 Gigabit or higher network is the best way to make these applications faster. This is especially true for “distributed reduce” applications such as group-bys, reduce-bys, and SQL joins. In any given application, you can see how much data Spark shuffles across the network from the application’s monitoring UI (http://<driver-node>:4040).

E. CPU: Spark scales well to tens of CPU cores per machine because it performs minimal sharing between threads. You should likely provision at least 8-16 cores per machine. Depending on the CPU cost of your workload, you may also need more: once data is in memory, most applications are either CPU- or network-bound.

====================================================



The spark.yarn.driver.memoryOverhead and spark.driver.cores values are derived from the resources of the node that AEL is installed on, under the assumption that only the driver executor is running there. The spark.default.parallelism value is derived from the amount of parallelism per core that is required (an arbitrary setting). In the example above, a value of 36 is derived from a parallelism per core setting of 2, multiplied by the spark.executor.instances, 18.

spark.executor.instances - 18
spark.yarn.executor.memoryOverhead - 1024MiB
spark.executor.memory - 2G
spark.yarn.driver.memoryOverhead - 1024MiB
spark.driver.memory - 3G
spark.executor.cores - 2
spark.driver.cores - 2
spark.default.parallelism - 18 executors * 2 core per executors = 36





